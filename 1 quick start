# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html

import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor

# Download training data from open datasets.
training_data = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor(),
)

# Download test data from open datasets.
test_data = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor(),
)

batch_size = 64

# Create data loaders.
train_dataloader = DataLoader(training_data, batch_size=batch_size)
test_dataloader = DataLoader(test_data, batch_size=batch_size)

for X, y in test_dataloader:
    print(f"Shape of X [N, C, H, W]: {X.shape}")
    print(f"Shape of y: {y.shape} {y.dtype}")
    break

# Get cpu, gpu or mps device for training.
device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using {device} device")

# Define model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork().to(device)
print(model)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)

def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        # Compute prediction error
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if batch % 100 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

def test(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")

epochs = 5
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train(train_dataloader, model, loss_fn, optimizer)
    test(test_dataloader, model, loss_fn)
print("Done!")

torch.save(model.state_dict(), "model.pth")
print("Saved PyTorch Model State to model.pth")

model = NeuralNetwork().to(device)
model.load_state_dict(torch.load("model.pth"))

classes = [
    "T-shirt/top",
    "Trouser",
    "Pullover",
    "Dress",
    "Coat",
    "Sandal",
    "Shirt",
    "Sneaker",
    "Bag",
    "Ankle boot",
]

model.eval()
x, y = test_data[0][0], test_data[0][1]
with torch.no_grad():
    x = x.to(device)
    pred = model(x)
    predicted, actual = classes[pred[0].argmax(0)], classes[y]
    print(f'Predicted: "{predicted}", Actual: "{actual}"')


’‘’
Importing necessary libraries (import statements):

This is like gathering all the ingredients and tools we need before opening our hamburger shop, such as buns, patties, vegetables, etc.
In the code, we import PyTorch libraries (torch), neural network modules (nn), data loaders (DataLoader), and computer vision datasets (datasets).


Downloading and preparing data (training_data and test_data):

This is like going out to buy the ingredients for our hamburgers and bringing them back to the shop.
In the code, we download the Fashion-MNIST dataset and transform it into a format that PyTorch can use (using ToTensor()).


Creating data loaders (train_dataloader and test_dataloader):

This is like dividing the ingredients into different trays, each containing a specific portion, making it easier to assemble the hamburgers later.
In the code, we split the dataset into batches, each containing 64 samples.


Choosing the device (device):

This is like deciding which kitchen we'll use to make the hamburgers: a regular kitchen (CPU) or a high-end kitchen (GPU or MPS).
In the code, we check if a GPU or MPS device is available; if not, we use the CPU.


Defining the neural network model (NeuralNetwork class):

This is like designing the assembly line for making hamburgers, deciding the order and combination of buns, patties, and vegetables.
In the code, we define a neural network with multiple layers of neurons to classify images.


Defining the loss function and optimizer (loss_fn and optimizer):

This is like choosing the criteria for judging the quality of our hamburgers (loss function) and the method for improving the recipe (optimizer).
In the code, we use the cross-entropy loss function (CrossEntropyLoss) and stochastic gradient descent optimizer (SGD).


Training the model (train function):

This is like repeatedly making hamburgers according to our designed process and adjusting the recipe based on customer feedback.
In the code, we iterate over the training dataset, feed the data into the model, calculate the loss, and update the model's parameters through backpropagation and the optimizer.


Testing the model (test function):

This is like having some taste testers try our hamburgers and give us scores and feedback.
In the code, we evaluate the model's performance using the test dataset, calculating accuracy and average loss.


Training and testing loop (epochs loop):

This is like practicing making hamburgers over and over, improving based on feedback until we reach a satisfactory level.
In the code, we perform 5 epochs of training and testing, with each epoch going through the entire training and test datasets.


Saving and loading the model (torch.save and torch.load):

This is like writing down our perfected hamburger recipe so that we can use it directly in the future.
In the code, we save the trained model's parameters to a file and reload them when needed.


Using the model for predictions (model.eval and model(x)):

This is like using our perfected recipe to make a hamburger and presenting it to a customer.
In the code, we use the trained model to predict the class of a single image and print the predicted and actual classes.
‘’‘
