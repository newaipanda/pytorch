# https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html

import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor

# Download training data from open datasets.
training_data = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor(),
)

# Download test data from open datasets.
test_data = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor(),
)

batch_size = 64

# Create data loaders.
train_dataloader = DataLoader(training_data, batch_size=batch_size)
test_dataloader = DataLoader(test_data, batch_size=batch_size)

for X, y in test_dataloader:
    print(f"Shape of X [N, C, H, W]: {X.shape}")
    print(f"Shape of y: {y.shape} {y.dtype}")
    break

# Get cpu, gpu or mps device for training.
device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using {device} device")

# Define model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork().to(device)
print(model)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)

def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        # Compute prediction error
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if batch % 100 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

def test(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")

epochs = 5
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train(train_dataloader, model, loss_fn, optimizer)
    test(test_dataloader, model, loss_fn)
print("Done!")

torch.save(model.state_dict(), "model.pth")
print("Saved PyTorch Model State to model.pth")

model = NeuralNetwork().to(device)
model.load_state_dict(torch.load("model.pth"))

classes = [
    "T-shirt/top",
    "Trouser",
    "Pullover",
    "Dress",
    "Coat",
    "Sandal",
    "Shirt",
    "Sneaker",
    "Bag",
    "Ankle boot",
]

model.eval()
x, y = test_data[0][0], test_data[0][1]
with torch.no_grad():
    x = x.to(device)
    pred = model(x)
    predicted, actual = classes[pred[0].argmax(0)], classes[y]
    print(f'Predicted: "{predicted}", Actual: "{actual}"')


’‘’
Importing necessary libraries (import statements):

This is like gathering all the ingredients and tools we need before opening our hamburger shop, such as buns, patties, vegetables, etc.
In the code, we import PyTorch libraries (torch), neural network modules (nn), data loaders (DataLoader), and computer vision datasets (datasets).


Downloading and preparing data (training_data and test_data):

This is like going out to buy the ingredients for our hamburgers and bringing them back to the shop.
In the code, we download the Fashion-MNIST dataset and transform it into a format that PyTorch can use (using ToTensor()).


Creating data loaders (train_dataloader and test_dataloader):

This is like dividing the ingredients into different trays, each containing a specific portion, making it easier to assemble the hamburgers later.
In the code, we split the dataset into batches, each containing 64 samples.


Choosing the device (device):

This is like deciding which kitchen we'll use to make the hamburgers: a regular kitchen (CPU) or a high-end kitchen (GPU or MPS).
In the code, we check if a GPU or MPS device is available; if not, we use the CPU.


Defining the neural network model (NeuralNetwork class):

This is like designing the assembly line for making hamburgers, deciding the order and combination of buns, patties, and vegetables.
In the code, we define a neural network with multiple layers of neurons to classify images.


Defining the loss function and optimizer (loss_fn and optimizer):

This is like choosing the criteria for judging the quality of our hamburgers (loss function) and the method for improving the recipe (optimizer).
In the code, we use the cross-entropy loss function (CrossEntropyLoss) and stochastic gradient descent optimizer (SGD).


Training the model (train function):

This is like repeatedly making hamburgers according to our designed process and adjusting the recipe based on customer feedback.
In the code, we iterate over the training dataset, feed the data into the model, calculate the loss, and update the model's parameters through backpropagation and the optimizer.


Testing the model (test function):

This is like having some taste testers try our hamburgers and give us scores and feedback.
In the code, we evaluate the model's performance using the test dataset, calculating accuracy and average loss.


Training and testing loop (epochs loop):

This is like practicing making hamburgers over and over, improving based on feedback until we reach a satisfactory level.
In the code, we perform 5 epochs of training and testing, with each epoch going through the entire training and test datasets.


Saving and loading the model (torch.save and torch.load):

This is like writing down our perfected hamburger recipe so that we can use it directly in the future.
In the code, we save the trained model's parameters to a file and reload them when needed.


Using the model for predictions (model.eval and model(x)):

This is like using our perfected recipe to make a hamburger and presenting it to a customer.
In the code, we use the trained model to predict the class of a single image and print the predicted and actual classes.
‘’‘

### V2
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor

# Download training data from open datasets.
training_data = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor(),
)

# Download test data from open datasets.
test_data = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor(),
)

batch_size = 128

# Create data loaders.
train_dataloader = DataLoader(training_data, batch_size=batch_size)
test_dataloader = DataLoader(test_data, batch_size=batch_size)

for X, y in test_dataloader:
    print(f"Shape of X [N, C, H, W]: {X.shape}")
    print(f"Shape of y: {y.shape} {y.dtype}")
    break

# Get cpu, gpu or mps device for training.
device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using {device} device")

# Define model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Dropout(p=0.2),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Dropout(p=0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork().to(device)
print(model)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        # Compute prediction error
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if batch % 100 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

def test(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")

epochs = 15
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train(train_dataloader, model, loss_fn, optimizer)
    test(test_dataloader, model, loss_fn)
print("Done!")

torch.save(model.state_dict(), "model.pth")
print("Saved PyTorch Model State to model.pth")

model = NeuralNetwork().to(device)
model.load_state_dict(torch.load("model.pth"))

classes = [
    "T-shirt/top",
    "Trouser",
    "Pullover",
    "Dress",
    "Coat",
    "Sandal",
    "Shirt",
    "Sneaker",
    "Bag",
    "Ankle boot",
]

model.eval()
x, y = test_data[0][0], test_data[0][1]
with torch.no_grad():
    x = x.to(device)
    pred = model(x)
    predicted, actual = classes[pred[0].argmax(0)], classes[y]
    print(f'Predicted: "{predicted}", Actual: "{actual}"')

# V2 Notes
# The optimized version of the code includes the following improvements:

# Increased the number of training epochs from 5 to 15.
# Used the Adam optimizer with a learning rate of 1e-3.
# Added an extra fully connected layer and ReLU activation layer to the neural network, making the model deeper.
# Added Dropout layers between the fully connected layers, randomly dropping 20% of the neurons to reduce overfitting.
# Increased the batch size to 128.

# output： Accuracy: 88.7%, Avg loss: 0.329483 ， VS 64% of version 1


# Changes for V2

# Before:
# batch_size = 64
# After:
batch_size = 128

# Before:
# model = NeuralNetwork().to(device)
# After:
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            # Added Dropout layer
            nn.Dropout(p=0.2),
            nn.Linear(512, 512),
            nn.ReLU(),
            # Added Dropout layer
            nn.Dropout(p=0.2),
            # Added extra linear and ReLU layers
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork().to(device)

# Before:
# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
# After:
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# Before:
# epochs = 5
# After:
epochs = 15




‘’‘
可解释性 (Interpretability): By providing clear explanations for each optimization step, such as increasing the number of epochs, using the Adam optimizer, adding layers, introducing Dropout, and increasing the batch size, we gain a better understanding of how these changes contribute to the model's performance improvement.
过拟合 (Overfitting): Overfitting occurs when a model learns the noise in the training data to the extent that it negatively impacts its performance on new data. By adding Dropout layers and not drastically increasing the number of epochs, we reduce the risk of overfitting and improve the model's generalization ability.
模型复杂度 (Model Complexity): Adding an extra fully connected layer and ReLU activation layer increases the model's complexity, allowing it to learn more intricate patterns in the data. However, we need to strike a balance to avoid making the model too complex, which could lead to overfitting.
优化器选择 (Optimizer Selection): Using the Adam optimizer, which adapts the learning rate for each parameter based on its historical gradients, can lead to faster convergence and better performance compared to the basic stochastic gradient descent (SGD) optimizer.
批次大小 (Batch Size): Increasing the batch size from 64 to 128 allows the model to process more samples in parallel, leading to faster training and potentially better generalization. However, the batch size should not be too large, as it may result in memory constraints and reduced model performance.
正则化 (Regularization): Dropout is a regularization technique that helps prevent overfitting by randomly dropping out neurons during training. By introducing Dropout layers, we reduce the model's reliance on specific neurons and force it to learn more robust features.
收敛速度 (Convergence Speed): The choice of optimizer (Adam) and the increased batch size contribute to faster convergence of the model during training. Faster convergence means the model reaches its optimal performance in fewer iterations.
泛化能力 (Generalization Ability): The combination of techniques like Dropout regularization, avoiding overfitting, and using an appropriate number of epochs helps improve the model's ability to generalize well to unseen data.
特征学习 (Feature Learning): The added fully connected layer and ReLU activation allow the model to learn more complex and hierarchical features from the input data. This enhanced feature learning capability contributes to better classification performance.
超参数调优 (Hyperparameter Tuning): The optimization process involves tuning hyperparameters such as the learning rate, batch size, and the number of epochs. Finding the right combination of hyperparameters is crucial for achieving optimal model performance.

These concepts can be further explained using the hamburger shop analogy:

可解释性 (Interpretability): Just as explaining each step in making a hamburger helps others understand the process better, providing explanations for each optimization step enhances the understanding of how the model's performance is improved.
过拟合 (Overfitting): Overfitting is like adding too many toppings to a hamburger, making it overly complex and less appealing to new customers. By avoiding overfitting, we ensure the model remains simple and generalizable.
模型复杂度 (Model Complexity): Adding layers to the model is like adding more ingredients to the hamburger. While it can enhance the flavor, too many ingredients can make the hamburger difficult to handle and less enjoyable.
优化器选择 (Optimizer Selection): Choosing the right optimizer is like selecting the best cooking method for the hamburger patties. The Adam optimizer is like using a grill with adjustable temperature control, ensuring even cooking and better results.
批次大小 (Batch Size): Increasing the batch size is like preparing more hamburgers simultaneously. It speeds up the process, but making too many at once may compromise quality and exceed the available resources.
正则化 (Regularization): Dropout regularization is like randomly removing some toppings from the hamburger during preparation. This ensures the hamburger tastes good even without relying on specific toppings, making it more robust.
收敛速度 (Convergence Speed): Faster convergence is like quickly assembling the hamburgers while maintaining quality. The right combination of techniques helps achieve the desired result efficiently.
泛化能力 (Generalization Ability): A model with good generalization ability is like a hamburger that appeals to a wide range of customers, even those who haven't tried it before.
特征学习 (Feature Learning): The added layers in the model are like incorporating new flavors and textures into the hamburger, making it more interesting and enjoyable.
超参数调优 (Hyperparameter Tuning): Tuning hyperparameters is like adjusting the recipe and cooking process to create the perfect hamburger that satisfies the customers' tastes.

Competition Standard
Accuracy: In many machine learning competitions, the accuracy of the model on the test dataset is the primary evaluation metric. The higher the accuracy, the higher the ranking. However, focusing solely on accuracy may overlook other important characteristics of the model.
Model Complexity: If the competition has constraints on model complexity, such as the number of parameters or model depth, simpler models with good performance may rank higher than more complex models. This encourages participants to design efficient and effective model architectures.
Inference Speed: In some scenarios, the inference speed of the model can be a critical consideration. If the competition requires the model to run in resource-constrained environments, such as mobile devices or embedded systems, models with faster inference speeds may rank higher.
Generalization Ability: The competition may evaluate the generalization ability of the models using a private test dataset that participants cannot access. In this case, models that overfit the training data may perform poorly in the rankings, while models that generalize well to new data will achieve higher rankings.
Innovation: Some competitions may place a special emphasis on the innovativeness and originality of the participants' approaches. Models that introduce novel techniques, architectures, or training methods may receive additional recognition and higher rankings.
Code Quality and Reproducibility: Competitions may require participants to submit their code for review. The quality, readability, and reproducibility of the code may impact the rankings. Good coding practices and detailed documentation can help participants achieve higher rankings.
Application of Domain Knowledge: If the competition is related to a specific domain, such as healthcare, finance, or natural language processing, models that successfully apply domain knowledge may outperform generic models. Demonstrating a deep understanding of the problem domain can boost rankings.
Model Interpretability: In certain domains, such as medical diagnosis or credit scoring, the interpretability of the model is crucial. If the competition requires participants to provide interpretable models, models with good interpretability may rank higher than "black-box" models.
’‘’

# Homework: 消融实验(Ablation Study) 
# 5 changes, so fix 4 , only make 1 changes and run , then compare all 5 cases

‘’‘
panda中文收藏
可解释性:通过为每个优化步骤提供清晰的解释,如增加epochs数、使用Adam优化器、添加层、引入Dropout和增加批次大小,我们可以更好地理解这些变化如何促进模型性能的提升。
过拟合:当模型过度学习训练数据中的噪声,以至于对新数据的性能产生负面影响时,就会发生过拟合。通过添加Dropout层和不大幅增加epochs数,我们降低了过拟合的风险,提高了模型的泛化能力。
模型复杂度:添加额外的全连接层和ReLU激活层增加了模型的复杂性,使其能够学习数据中更复杂的模式。然而,我们需要取得平衡,避免使模型过于复杂,这可能导致过拟合。
优化器选择:使用Adam优化器可以根据每个参数的历史梯度调整学习率,与基本的随机梯度下降(SGD)优化器相比,可以加快收敛速度并获得更好的性能。
批次大小:将批次大小从64增加到128,允许模型并行处理更多样本,从而加快训练速度,并有可能提高泛化能力。但是,批次大小不应太大,因为它可能导致内存限制和模型性能下降。
正则化:Dropout是一种正则化技术,通过在训练过程中随机丢弃神经元来防止过拟合。通过引入Dropout层,我们减少了模型对特定神经元的依赖,迫使其学习更鲁棒的特征。
收敛速度:优化器(Adam)的选择和增加的批次大小有助于加快模型在训练过程中的收敛速度。收敛速度越快,意味着模型在更少的迭代中达到最佳性能。
泛化能力:Dropout正则化、避免过拟合以及使用适当数量的epochs等技术的结合,有助于提高模型在未见过的数据上的泛化能力。
特征学习:添加的全连接层和ReLU激活允许模型从输入数据中学习更复杂和层次化的特征。这种增强的特征学习能力有助于提高分类性能。
超参数调优:优化过程涉及调整学习率、批次大小和epochs数等超参数。找到超参数的正确组合对于实现模型的最佳性能至关重要。

这些概念可以用汉堡店的类比进一步解释:

可解释性:就像解释制作汉堡的每个步骤有助于其他人更好地理解这个过程一样,为每个优化步骤提供解释可以增强对模型性能如何提升的理解。
过拟合:过拟合就像在汉堡上添加太多配料,使其过于复杂,对新客户的吸引力降低。通过避免过拟合,我们确保模型保持简单和可推广。
模型复杂度:向模型添加层就像在汉堡中添加更多配料。虽然它可以增强口味,但太多配料会使汉堡难以处理且不太美味。
优化器选择:选择正确的优化器就像为汉堡肉饼选择最佳的烹饪方法。Adam优化器就像使用具有可调温度控制的烤架,确保烹饪均匀,效果更好。
批次大小:增加批次大小就像同时制作更多汉堡。它加快了制作过程,但一次制作太多可能会影响质量并超出可用资源。
正则化:Dropout正则化就像在制作过程中随机去除汉堡中的一些配料。这确保即使不依赖特定配料,汉堡也会味道不错,从而使其更加鲁棒。
收敛速度:更快的收敛就像在保证质量的同时快速组装汉堡。正确的技术组合有助于高效地达到预期结果。
泛化能力:具有良好泛化能力的模型就像一个汉堡,即使是从未尝试过的顾客也会喜欢。
特征学习:模型中添加的层就像在汉堡中加入新的口味和质地,使其更有趣、更美味。
超参数调优:调整超参数就像调整配方和烹饪过程,制作出完美的汉堡,满足顾客的口味。

比赛评判因素
准确率 (Accuracy): 在许多机器学习比赛中,模型在测试数据集上的准确率是主要的评判标准。准确率越高,排名越靠前。然而,仅关注准确率可能会忽略模型的其他重要特性。
模型复杂度 (Model Complexity): 如果比赛对模型的复杂度有所限制,如参数数量或模型深度,则较简单但性能良好的模型可能比复杂的模型排名更高。这鼓励参赛者设计高效且有效的模型架构。
推理速度 (Inference Speed): 在某些场景下,模型的推理速度可能是一个重要的考量因素。如果比赛要求模型在资源有限的环境中运行,如移动设备或嵌入式系统,则推理速度更快的模型可能排名更高。
泛化能力 (Generalization Ability): 比赛可能会使用参赛者无法访问的私有测试数据集来评估模型的泛化能力。在这种情况下,过拟合训练数据的模型在排名中可能表现不佳,而能够很好地泛化到新数据的模型则会获得更高的排名。
创新性 (Innovation): 一些比赛可能会特别关注参赛者的创新性和原创性。引入新颖的技术、架构或训练方法的模型可能会获得额外的认可和更高的排名。
代码质量和可复现性 (Code Quality and Reproducibility): 比赛可能要求参赛者提交他们的代码以供评审。代码的质量、可读性和可复现性可能会影响排名。良好的代码实践和详细的文档可以帮助参赛者获得更高的排名。
领域知识的应用 (Domain Knowledge Application): 如果比赛与特定领域相关,如医疗、金融或自然语言处理,则成功应用领域知识的模型可能比通用模型表现更好。展示对问题领域的深入理解可以提高排名。
模型的可解释性 (Model Interpretability): 在某些领域,如医疗诊断或信用评分,模型的可解释性非常重要。如果比赛要求参赛者提供可解释的模型,则具有良好可解释性的模型可能比"黑盒"模型排名更高。
‘’‘
